{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the 2L-SPC on MNIST database\n",
    "https://arxiv.org/abs/2002.00892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str((Path(\"..\").resolve().absolute())))\n",
    "\n",
    "from  SPC_2L.DataTools import DataBase\n",
    "from SPC_2L.Network import LayerPC, Network\n",
    "from SPC_2L.Coding import ML_Lasso,ML_FISTA\n",
    "from SPC_2L.DataTools import DataBase, gaussian_kernel\n",
    "from SPC_2L.Monitor import Monitor\n",
    "from SPC_2L.Optimizers import mySGD, myAdam\n",
    "import torch.nn.functional as f\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import tensorboardX\n",
    "from SPC_2L.DataTools import LCN, whitening, z_score, mask, to_cuda, norm\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import pickle\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "#from SDPC.Monitor import Monitor\n",
    "import math\n",
    "from torchvision.datasets import MNIST\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, parameters and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Loading parameters\n",
    "file = open('config.json',)\n",
    "config = json.load(file)\n",
    "\n",
    "## Database \n",
    "data_path = '../../DataSet/MNIST/'\n",
    "\n",
    "resize = Resize(tuple(config['Transforms']['Resize_Size'])) if config['Transforms']['Use_Resize'] else None\n",
    "whiten = whitening(tuple(config['Transforms']['Whitening_Size']), f_0=config['Transforms']['Whitening_f_0']) if config['Transforms']['Use_Whitening'] else None\n",
    "lcn = LCN(config['Transforms']['LCN_Kernel_Size'], config['Transforms']['LCN_Sigma'], config['Transforms']['LCN_RGB']) if config['Transforms']['Use_LCN'] else None\n",
    "zscore = z_score() if config['Transforms']['Use_z_score'] else None\n",
    "Mask = mask(tuple(config['Transforms']['Mask_Size'])) if config['Transforms']['Use_Mask'] else None\n",
    "\n",
    "transform_list = [transform for transform in [ToTensor(), to_cuda(), resize, whiten, lcn, zscore, Mask] if transform is not None]\n",
    "transform = Compose(transform_list)\n",
    "\n",
    "dataset = MNIST(data_path, transform=transform, train=True, download=True)\n",
    "\n",
    "DataBase =  DataLoader(dataset,batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "## Gaussian masks for the dictionaries\n",
    "#if config.getboolean('Gaussian', 'Use_Gaussian'):\n",
    "    #mask_g = [gaussian_kernel((), sigma=config.getint('Gaussian', 'Gaussian_Sigma').split(\", \")[i]))]\n",
    "    #mask_g = [gaussian_kernel((64,3,8,8), sigma=30), gaussian_kernel((128,64,8,8), sigma=30)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving the network (b=0 for Hi-La, b=1 for 2L-SPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK STRUCTURE : \n",
      " Input : (32, 1, 28, 28)\n",
      " Layer 1 : [32, 32, 12, 12]\n",
      " Layer 2 : [32, 64, 8, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\SDPC\\SDPC\\SPC_2L\\Optimizers.py:73: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\SDPC\\SDPC\\notebooks\\1-MNIST_training_config.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx_batch, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(DataBase):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     batch \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     gamma, it, Loss_G, delta \u001b[39m=\u001b[39m Pursuit\u001b[39m.\u001b[39;49mcoding(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Net\u001b[39m.\u001b[39mnb_layers):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         Net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mdico\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mE:\\SDPC\\SDPC\\SPC_2L\\Coding.py:116\u001b[0m, in \u001b[0;36mML_FISTA.coding\u001b[1;34m(self, X, flag, gamma_in, softmax, labels)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m flag[i]:\n\u001b[0;32m    114\u001b[0m     gamma_z[i]\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     Loss[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLF\u001b[39m.\u001b[39;49mF(X, gamma_z, i, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m    117\u001b[0m     Loss[i]\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    119\u001b[0m     gamma_z[i]\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mE:\\SDPC\\SDPC\\SPC_2L\\Coding.py:189\u001b[0m, in \u001b[0;36mML_Lasso.F\u001b[1;34m(self, X, gamma, i, do_feedback, labels, softmax)\u001b[0m\n\u001b[0;32m    187\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbackward(f\u001b[39m.\u001b[39msoftmax(gamma[i],  dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m--> 189\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i]\u001b[39m.\u001b[39;49mbackward(gamma[i])\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    192\u001b[0m     Loss \u001b[39m=\u001b[39m (X \u001b[39m-\u001b[39m pred)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39ma \u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm[i])\n",
      "File \u001b[1;32mE:\\SDPC\\SDPC\\SPC_2L\\Network.py:92\u001b[0m, in \u001b[0;36mLayerPC.backward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(\u001b[39mself\u001b[39m,x)\n\u001b[1;32m---> 92\u001b[0m x \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mconv_transpose2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdico, stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad, output_padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_pad, groups\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Definition of the layers, network and sparse coding algorithm\n",
    "layer = [LayerPC((config[\"Network\"][\"Num_Features\"][i+1], config[\"Network\"][\"Num_Features\"][i], config[\"Params\"][\"Dico_Shape\"][0], config[\"Params\"][\"Dico_Shape\"][1]),\n",
    "                  stride=config[\"Params\"][\"Stride\"][i], \n",
    "                  b=config[\"Network\"][\"Feedback_Str\"], \n",
    "                  v=config[\"Network\"][\"Initial_Norm\"][i], \n",
    "                  v_size=config[\"Params\"][\"v_size\"][i], \n",
    "                  out_pad=config[\"Params\"][\"Out_Padding\"][i]) \n",
    "                  for i in range (config[\"Network\"][\"Num_Layers\"])]\n",
    "\n",
    "Net = Network(layer, input_size=(config[\"Network\"][\"Num_Features\"][1], config[\"Network\"][\"Num_Features\"][0], config[\"Params\"][\"Image_Size\"][0], config[\"Params\"][\"Image_Size\"][1]))\n",
    "Loss = ML_Lasso(Net, [config[\"Network\"][\"Sparsity_Param\"][i] for i in range(config[\"Network\"][\"Num_Layers\"])])\n",
    "Pursuit = ML_FISTA(Net, Loss, max_iter=1000, th=5e-4, mode='eigen')\n",
    "\n",
    "# Optimizer initialization\n",
    "opt_dico = [None] * (Net.nb_layers + 1)\n",
    "for i in range(0, Net.nb_layers):\n",
    "    opt_dico[i] = mySGD([{'params': Net.layers[i].dico}], lr=config[\"Network\"][\"Dictionary_Lr\"][i], momentum=0.9, normalize=True)\n",
    "\n",
    "opt_v = [myAdam([{'params': Net.layers[i].v}], lr=config[\"Network\"][\"Normalizer_Lr\"][i], normalize=False) for i in range(Net.nb_layers)]\n",
    "\n",
    "\n",
    "L = [None] * (Net.nb_layers)\n",
    "L_v = [None] * (Net.nb_layers)\n",
    "reco = [None] * (Net.nb_layers)\n",
    "\n",
    "model_name = 'MNIST_[{0},{1}]_b={2}'.format(config[\"Network\"][\"Sparsity_Param\"][0], config[\"Network\"][\"Sparsity_Param\"][1], config[\"Network\"][\"Feedback_Str\"])\n",
    "path = 'Savings/MNIST/' + model_name +'.pkl'\n",
    "if config[\"Params\"][\"Use_Tensorb\"]:\n",
    "    nrows = [8,8,8,8,8,8,8]\n",
    "    writer = SummaryWriter('Savings/Log/' + model_name)\n",
    "    M = Monitor(Net, writer, n_row=nrows)\n",
    "\n",
    "k=0\n",
    "\n",
    "l2_loss = torch.zeros(2, config[\"Params\"][\"Num_Epoch\"] * len(DataBase))\n",
    "l1_loss = torch.zeros(2, config[\"Params\"][\"Num_Epoch\"] * len(DataBase))\n",
    "if config[\"Params\"][\"Save\"]: \n",
    "    for e in range(config[\"Params\"][\"Num_Epoch\"]):\n",
    "        for idx_batch, data in enumerate(DataBase):\n",
    "\n",
    "            batch = data[0].cuda()\n",
    "            gamma, it, Loss_G, delta = Pursuit.coding(batch)\n",
    "\n",
    "\n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].dico.requires_grad = True\n",
    "                L[i] = Loss.F(batch, gamma, i, do_feedback=False).div(batch.size()[0])  ## Unsupervised\n",
    "                L[i].backward()\n",
    "                Net.layers[i].dico.requires_grad = False\n",
    "                opt_dico[i].step()\n",
    "                opt_dico[i].zero_grad()\n",
    "                \n",
    "                l2_loss[i,k]= L[i].detach()\n",
    "                l1_loss[i,k] =  gamma[i].detach().sum().div(gamma[i].size(0))\n",
    "                \n",
    "\n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].v.requires_grad = True  # turn_on(i)\n",
    "                L_v[i] = Loss.F_v(batch, gamma, i).div(batch.size()[0])\n",
    "                L_v[i].backward()\n",
    "                Net.layers[i].v.requires_grad = False  # turn_off(i)\n",
    "                opt_v[i].step()  \n",
    "                opt_v[i].zero_grad()\n",
    "                \n",
    "            if config[\"Params\"][\"Use_Tensorb\"]:\n",
    "                if (k%10) == 0:\n",
    "                    writer.add_scalar('FISTA_iterations', it, k)\n",
    "                    M.MonitorGamma(gamma, k, option=['NNZ', '%', 'Sum', 'V'])\n",
    "                    M.MonitorList(L, 'Loss_Dico', k)\n",
    "                    M.MonitorList(L_v, 'Loss_v', k)\n",
    "                    M.MonitorDicoBP(k)\n",
    "                    M.ComputeHisto(gamma)\n",
    "\n",
    "                if (k%100) == 0:\n",
    "                    reco = [None] * (Net.nb_layers)\n",
    "                    for i in range(Net.nb_layers-1,-1,-1):\n",
    "                        reco[i] = gamma[i]\n",
    "                        for j in range(i, -1, -1):\n",
    "                            reco[i] = Net.layers[j].backward(reco[i])\n",
    "                        reco_image = make_grid(reco[i],normalize=True,pad_value=1)\n",
    "                        writer.add_image('Reco/L{0}'.format(i),reco_image,k)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "    output_exp = {'Net': Net,\n",
    "            'Loss': Loss,\n",
    "            'Pursuit': Pursuit,\n",
    "            'l2_loss': l2_loss,\n",
    "            'l1_loss': l1_loss    \n",
    "                 }\n",
    "    \n",
    "    \n",
    "else :        \n",
    "    with open(path, 'rb') as file:\n",
    "        output_exp = pickle.load(file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Savings/MNIST/' + model_name +'.pkl'\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(output_exp, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(data_path, transform=transform, train=False, download=True)\n",
    "\n",
    "DataBaseT =  DataLoader(dataset,batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "for idx_batch, data in enumerate(DataBaseT):\n",
    "\n",
    "        batch = data[0].cuda()\n",
    "        gamma, it, Loss_G, delta = Pursuit.coding(batch)\n",
    "\n",
    "\n",
    "        for i in range(Net.nb_layers):\n",
    "            Net.layers[i].dico.requires_grad = True\n",
    "            L[i] = Loss.F(batch, gamma, i, do_feedback=False).div(batch.size()[0])  ## Unsupervised\n",
    "            Net.layers[i].dico.requires_grad = False\n",
    "            L[i].backward()\n",
    "            opt_dico[i].step()\n",
    "            opt_dico[i].zero_grad()\n",
    "            \n",
    "            l2_loss[i,k]= L[i].detach() \n",
    "            l1_loss[i,k] =  gamma[i].detach().sum().div(gamma[i].size(0))\n",
    "                \n",
    "\n",
    "        for i in range(Net.nb_layers):\n",
    "            Net.layers[i].v.requires_grad = True  # turn_on(i)\n",
    "            L_v[i] = Loss.F_v(batch, gamma, i).div(batch.size()[0])\n",
    "            L_v[i].backward()\n",
    "            Net.layers[i].v.requires_grad = False  # turn_off(i)\n",
    "            opt_v[i].step()  \n",
    "            opt_v[i].zero_grad()\n",
    "                \n",
    "        if config[\"Params\"][\"Use_Tensorb\"]:\n",
    "            if (k%10) == 0:\n",
    "                writer.add_scalar('FISTA_iterations', it, k)\n",
    "                M.MonitorGamma(gamma, k, option=['NNZ', '%', 'Sum', 'V'])\n",
    "                M.MonitorList(L, 'Loss_Dico', k)\n",
    "                M.MonitorList(L_v, 'Loss_v', k)\n",
    "                M.MonitorDicoBP(k)\n",
    "                M.ComputeHisto(gamma)\n",
    "\n",
    "            if (k%100) == 0:\n",
    "                reco = [None] * (Net.nb_layers)\n",
    "                for i in range(Net.nb_layers-1,-1,-1):\n",
    "                    reco[i] = gamma[i]\n",
    "                    for j in range(i, -1, -1):\n",
    "                        reco[i] = Net.layers[j].backward(reco[i])\n",
    "                    reco_image = make_grid(reco[i],normalize=True,pad_value=1)\n",
    "                    writer.add_image('Reco/L{0}'.format(i),reco_image,k)\n",
    "                    \n",
    "        k += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
