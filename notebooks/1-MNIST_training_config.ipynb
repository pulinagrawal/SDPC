{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the 2L-SPC on MNIST database\n",
    "https://arxiv.org/abs/2002.00892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str((Path(\"..\").resolve().absolute())))\n",
    "\n",
    "from  SPC_2L.DataTools import DataBase\n",
    "from SPC_2L.Network import LayerPC, Network\n",
    "from SPC_2L.Coding import ML_Lasso,ML_FISTA\n",
    "from SPC_2L.DataTools import DataBase, gaussian_kernel\n",
    "from SPC_2L.Monitor import Monitor\n",
    "from SPC_2L.Optimizers import mySGD, myAdam\n",
    "import torch.nn.functional as f\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import tensorboardX\n",
    "from SPC_2L.DataTools import LCN, whitening, z_score, mask, to_cuda, norm\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import pickle\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "#from SDPC.Monitor import Monitor\n",
    "import math\n",
    "from torchvision.datasets import MNIST\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, parameters and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Loading parameters\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Database \n",
    "data_path = '../../DataSet/MNIST/'\n",
    "\n",
    "resize = Resize((int(config.get('Transforms', 'Resize_Size').split(\", \")[0]),int(config.get('Transforms', 'Resize_Size').split(\", \")[1]))) if config.getboolean('Transforms', 'Use_Resize') else None\n",
    "whiten = whitening((int(config.get('Transforms', 'Whitening_Size').split(\", \")[0]),int(config.get('Transforms', 'Whitening_Size').split(\", \")[1])),f_0=config.getfloat('Transforms', 'Whitening_f_0')) if config.getboolean('Transforms', 'Use_Whitening') else None\n",
    "lcn = LCN(config.getint('Transforms', 'LCN_Kernel_Size'),config.getfloat('Transforms', 'LCN_Sigma'), config.getboolean('Transforms', 'LCN_RGB')) if config.getboolean('Transforms', 'Use_LCN') else None\n",
    "zscore = z_score() if config.getboolean('Transforms', 'Use_z_score') else None\n",
    "Mask = mask((int(config.get('Transforms', 'Mask_Size').split(\", \")[0]),int(config.get('Transforms', 'Mask_Size').split(\", \")[1]))) if config.getboolean('Transforms', 'Use_Mask') else None\n",
    "\n",
    "transform_list = [transform for transform in [ToTensor(), to_cuda(), resize, whiten, lcn, zscore, Mask] if transform is not None]\n",
    "transform = Compose(transform_list)\n",
    "\n",
    "dataset = MNIST(data_path, transform=transform, train=True, download=True)\n",
    "\n",
    "DataBase =  DataLoader(dataset,batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "## Gaussian masks for the dictionaries\n",
    "#if config.getboolean('Gaussian', 'Use_Gaussian'):\n",
    "    #mask_g = [gaussian_kernel((), sigma=config.getint('Gaussian', 'Gaussian_Sigma').split(\", \")[i]))]\n",
    "    #mask_g = [gaussian_kernel((64,3,8,8), sigma=30), gaussian_kernel((128,64,8,8), sigma=30)]\n",
    "#??? where does the 8 come from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving the network (b=0 for Hi-La, b=1 for 2L-SPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK STRUCTURE : \n",
      " Input : (32, 1, 28, 28)\n",
      " Layer 1 : [32, 32, 12, 12]\n",
      " Layer 2 : [32, 64, 8, 8]\n",
      "0.2 0.3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\SDPC\\SDPC\\SPC_2L\\Optimizers.py:73: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\SDPC\\SDPC\\notebooks\\1-MNIST_training_config.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx_batch, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(DataBase):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     batch \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     gamma, it, Loss_G, delta \u001b[39m=\u001b[39m Pursuit\u001b[39m.\u001b[39;49mcoding(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Net\u001b[39m.\u001b[39mnb_layers):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/SDPC/SDPC/notebooks/1-MNIST_training_config.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         Net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mdico\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mE:\\SDPC\\SDPC\\SPC_2L\\Coding.py:117\u001b[0m, in \u001b[0;36mML_FISTA.coding\u001b[1;34m(self, X, flag, gamma_in, softmax, labels)\u001b[0m\n\u001b[0;32m    114\u001b[0m gamma_z[i]\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    116\u001b[0m Loss[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLF\u001b[39m.\u001b[39mF(X, gamma_z, i, labels\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m--> 117\u001b[0m Loss[i]\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    119\u001b[0m gamma_z[i]\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m grad \u001b[39m=\u001b[39m gamma_z[i]\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\sdpc\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\sdpc\\lib\\site-packages\\torch\\autograd\\__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    236\u001b[0m     (inputs,)\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\sdpc\\lib\\site-packages\\torch\\autograd\\__init__.py:127\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n\u001b[0;32m    125\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    126\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 127\u001b[0m         torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Definition of the layers, network and sparse coding algorithm\n",
    "layer = [LayerPC((int(config.get(\"Params\", \"Num_Features\").split(\", \")[i+1]), int(config.get(\"Params\", \"Num_Features\").split(\", \")[i]), int(config.get(\"Params\", \"Dico_Shape\").split(\", \")[0]), int(config.get(\"Params\", \"Dico_Shape\").split(\", \")[1])),\n",
    "                  stride=int(config.get(\"Params\", \"Stride\").split(\", \")[i]), \n",
    "                  b=config.getint(\"Params\", \"Feedback_Str\"), \n",
    "                  v=int(config.get(\"Params\", \"Initial_Norm\").split(\", \")[i]), \n",
    "                  v_size=int(config.get(\"Params\", \"v_size\").split(\", \")[i]), \n",
    "                  out_pad=int(config.get(\"Params\", \"Out_Padding\").split(\", \")[i])) \n",
    "                  for i in range (config.getint(\"Params\", \"Num_Layers\"))]\n",
    "\n",
    "Net = Network(layer, input_size=(int(config.get(\"Params\", \"Num_Features\").split(\", \")[1]), int(config.get(\"Params\", \"Num_Features\").split(\", \")[0]), int(config.get(\"Params\", \"Image_Size\").split(\", \")[0]),int(config.get(\"Params\", \"Image_Size\").split(\", \")[1])))\n",
    "Loss = ML_Lasso(Net, [float(config.get(\"Params\", \"Sparsity_Param\").split(\", \")[i]) for i in range(config.getint(\"Params\", \"Num_Layers\"))])\n",
    "Pursuit = ML_FISTA(Net, Loss, max_iter=1000, th=5e-4, mode='eigen')\n",
    "\n",
    "## Optimizer initialization\n",
    "opt_dico = [None] * (Net.nb_layers + 1)\n",
    "for i in range(0, Net.nb_layers):\n",
    "    opt_dico[i] = mySGD([{'params': Net.layers[i].dico}], lr=float(config.get(\"Params\", \"Dictionary_Lr\").split(\", \")[i]), momentum=0.9, normalize=True)\n",
    "\n",
    "opt_v = [myAdam([{'params': Net.layers[i].v}], lr=float(config.get(\"Params\", \"Normalizer_Lr\").split(\", \")[i]), normalize=False) \\\n",
    "    for i in range(Net.nb_layers)]\n",
    "\n",
    "\n",
    "L = [None] * (Net.nb_layers)\n",
    "L_v = [None] * (Net.nb_layers)\n",
    "reco = [None] * (Net.nb_layers)\n",
    "\n",
    "model_name = 'MNIST_[{0},{1}]_b={2}'.format(float(config.get(\"Params\", \"Sparsity_Param\").split(\", \")[0]),float(config.get(\"Params\", \"Sparsity_Param\").split(\", \")[1]),config.getint(\"Params\", \"Feedback_Str\"))\n",
    "path = 'Savings/MNIST/' + model_name +'.pkl'\n",
    "if config.getboolean(\"Params\", \"Use_Tensorb\") : \n",
    "    nrows = [8,8,8,8,8,8,8]\n",
    "    writer = SummaryWriter('Savings/Log/' + model_name)\n",
    "    M = Monitor(Net, writer, n_row=nrows)\n",
    "\n",
    "k=0\n",
    "\n",
    "l2_loss = torch.zeros(2,config.getint(\"Params\", \"Num_Epoch\")*len(DataBase))\n",
    "l1_loss = torch.zeros(2,config.getint(\"Params\", \"Num_Epoch\")*len(DataBase)) #!!!! how do i replicate this for multiple layers\n",
    "if config.getboolean(\"Params\", \"Save\") == True : \n",
    "    for e in range(config.getint(\"Params\", \"Num_Epoch\")):\n",
    "        for idx_batch, data in enumerate(DataBase):\n",
    "\n",
    "            batch = data[0].cuda()\n",
    "            gamma, it, Loss_G, delta = Pursuit.coding(batch)\n",
    "\n",
    "\n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].dico.requires_grad = True\n",
    "                L[i] = Loss.F(batch, gamma, i, do_feedback=False).div(batch.size()[0])  ## Unsupervised\n",
    "                L[i].backward()\n",
    "                Net.layers[i].dico.requires_grad = False\n",
    "                opt_dico[i].step()\n",
    "                opt_dico[i].zero_grad()\n",
    "                \n",
    "                l2_loss[i,k]= L[i].detach() #!!!! with different calculations needed how to i increase past 2\n",
    "                l1_loss[i,k] =  gamma[i].detach().sum().div(gamma[i].size(0))\n",
    "                \n",
    "\n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].v.requires_grad = True  # turn_on(i)\n",
    "                L_v[i] = Loss.F_v(batch, gamma, i).div(batch.size()[0])\n",
    "                L_v[i].backward()\n",
    "                Net.layers[i].v.requires_grad = False  # turn_off(i)\n",
    "                opt_v[i].step()  \n",
    "                opt_v[i].zero_grad()\n",
    "                \n",
    "            if config.getboolean(\"Params\", \"Use_Tensorb\"):\n",
    "                if (k%10) == 0:\n",
    "                    writer.add_scalar('FISTA_iterations', it, k)\n",
    "                    M.MonitorGamma(gamma, k, option=['NNZ', '%', 'Sum', 'V'])\n",
    "                    M.MonitorList(L, 'Loss_Dico', k)\n",
    "                    M.MonitorList(L_v, 'Loss_v', k)\n",
    "                    M.MonitorDicoBP(k)\n",
    "                    M.ComputeHisto(gamma)\n",
    "\n",
    "                if (k%100) == 0:\n",
    "                    reco = [None] * (Net.nb_layers)\n",
    "                    for i in range(Net.nb_layers-1,-1,-1):\n",
    "                        reco[i] = gamma[i]\n",
    "                        for j in range(i, -1, -1):\n",
    "                            reco[i] = Net.layers[j].backward(reco[i])\n",
    "                        reco_image = make_grid(reco[i],normalize=True,pad_value=1)\n",
    "                        writer.add_image('Reco/L{0}'.format(i),reco_image,k)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "    output_exp = {'Net': Net,\n",
    "            'Loss': Loss,\n",
    "            'Pursuit': Pursuit,\n",
    "            'l2_loss': l2_loss,\n",
    "            'l1_loss': l1_loss    \n",
    "                 }\n",
    "    \n",
    "    \n",
    "else :        \n",
    "    with open(path, 'rb') as file:\n",
    "        output_exp = pickle.load(file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Savings/MNIST/' + model_name +'.pkl'\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(output_exp, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(data_path, transform=transform, train=False, download=True)\n",
    "\n",
    "DataBaseT =  DataLoader(dataset,batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "for idx_batch, data in enumerate(DataBaseT):\n",
    "\n",
    "        batch = data[0].cuda()\n",
    "        gamma, it, Loss_G, delta = Pursuit.coding(batch)\n",
    "\n",
    "\n",
    "        for i in range(Net.nb_layers):\n",
    "            Net.layers[i].dico.requires_grad = True\n",
    "            L[i] = Loss.F(batch, gamma, i, do_feedback=False).div(batch.size()[0])  ## Unsupervised\n",
    "            Net.layers[i].dico.requires_grad = False\n",
    "            L[i].backward()\n",
    "            opt_dico[i].step()\n",
    "            opt_dico[i].zero_grad()\n",
    "            \n",
    "            l2_loss[i,k]= L[i].detach() \n",
    "            l1_loss[i,k] =  gamma[i].detach().sum().div(gamma[i].size(0))\n",
    "                \n",
    "\n",
    "        for i in range(Net.nb_layers):\n",
    "            Net.layers[i].v.requires_grad = True  # turn_on(i)\n",
    "            L_v[i] = Loss.F_v(batch, gamma, i).div(batch.size()[0])\n",
    "            L_v[i].backward()\n",
    "            Net.layers[i].v.requires_grad = False  # turn_off(i)\n",
    "            opt_v[i].step()  \n",
    "            opt_v[i].zero_grad()\n",
    "                \n",
    "        if config[\"Params\"][\"Use_Tensorb\"]:\n",
    "            if (k%10) == 0:\n",
    "                writer.add_scalar('FISTA_iterations', it, k)\n",
    "                M.MonitorGamma(gamma, k, option=['NNZ', '%', 'Sum', 'V'])\n",
    "                M.MonitorList(L, 'Loss_Dico', k)\n",
    "                M.MonitorList(L_v, 'Loss_v', k)\n",
    "                M.MonitorDicoBP(k)\n",
    "                M.ComputeHisto(gamma)\n",
    "\n",
    "            if (k%100) == 0:\n",
    "                reco = [None] * (Net.nb_layers)\n",
    "                for i in range(Net.nb_layers-1,-1,-1):\n",
    "                    reco[i] = gamma[i]\n",
    "                    for j in range(i, -1, -1):\n",
    "                        reco[i] = Net.layers[j].backward(reco[i])\n",
    "                    reco_image = make_grid(reco[i],normalize=True,pad_value=1)\n",
    "                    writer.add_image('Reco/L{0}'.format(i),reco_image,k)\n",
    "                    \n",
    "        k += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
